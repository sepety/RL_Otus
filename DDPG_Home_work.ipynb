{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sepety/RL_Otus/blob/main/DDPG_Home_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DDPG (Deep Deterministic Policy Gradient)**"
      ],
      "metadata": {
        "id": "dt9CQGHU9s5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DDPG (Deep Deterministic Policy Gradient) — это алгоритм обучения с подкреплением (RL), который подходит для сред с непрерывным пространством действий. Он относится к семейству методов Actor-Critic, где у нас есть две основные сети:\n",
        "\n",
        "Actor (политика): аппроксимирует детерминированную стратегию\n",
        "𝜇\n",
        "(\n",
        "𝑠\n",
        "∣\n",
        "𝜃\n",
        "𝜇\n",
        ")\n",
        "μ(s∣θ\n",
        "μ\n",
        "​\n",
        " ), которая по состоянию\n",
        "𝑠\n",
        "s выдаёт действие\n",
        "𝑎\n",
        "a.\n",
        "Critic (ценностная функция): аппроксимирует функцию Q-value,\n",
        "𝑄\n",
        "(\n",
        "𝑠\n",
        ",\n",
        "𝑎\n",
        "∣\n",
        "𝜃\n",
        "𝑄\n",
        ")\n",
        "Q(s,a∣θ\n",
        "Q\n",
        "​\n",
        " ), которая оценивает качество действия\n",
        "𝑎\n",
        "a в состоянии\n",
        "𝑠\n",
        "s.\n"
      ],
      "metadata": {
        "id": "pD6-RGwU8puu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Математические основы**"
      ],
      "metadata": {
        "id": "5zKFm5s7876H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Целевая функция для Critic:\n",
        "Если агент исполняет стратегию\n",
        "𝜇\n",
        "μ, для обновления критика мы используем приближение целевого Q-значения:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝑟\n",
        "+\n",
        "𝛾\n",
        "𝑄\n",
        "′\n",
        "(\n",
        "𝑠\n",
        "′\n",
        ",\n",
        "𝜇\n",
        "′\n",
        "(\n",
        "𝑠\n",
        "′\n",
        ")\n",
        "∣\n",
        "𝜃\n",
        "𝑄\n",
        "′\n",
        ")\n",
        ".\n",
        "y=r+γQ\n",
        "′\n",
        " (s\n",
        "′\n",
        " ,μ\n",
        "′\n",
        " (s\n",
        "′\n",
        " )∣θ\n",
        "Q\n",
        "′\n",
        "\n",
        "​\n",
        " ).\n",
        "Здесь:\n",
        "\n",
        "𝑟\n",
        "r — награда за текущее действие\n",
        "𝛾\n",
        "γ — коэффициент дисконтирования (обычно\n",
        "𝛾\n",
        "∈\n",
        "[\n",
        "0.9\n",
        ",\n",
        "0.99\n",
        "]\n",
        "γ∈[0.9,0.99])\n",
        "𝑄\n",
        "′\n",
        "Q\n",
        "′\n",
        "  и\n",
        "𝜇\n",
        "′\n",
        "μ\n",
        "′\n",
        "  — целевые сети критика и актёра, служащие для стабилизации обучения.\n",
        "Критик обновляется путём минимизации ошибки МСЕ между предсказанным\n",
        "𝑄\n",
        "(\n",
        "𝑠\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "Q(s,a) и целевым значением\n",
        "𝑦\n",
        "y.\n",
        "\n",
        "Обновление Actor:\n",
        "Actor обновляется посредством политики градиента:\n",
        "\n",
        "∇\n",
        "𝜃\n",
        "𝜇\n",
        "𝐽\n",
        "≈\n",
        "𝐸\n",
        "[\n",
        "∇\n",
        "𝑎\n",
        "𝑄\n",
        "(\n",
        "𝑠\n",
        ",\n",
        "𝑎\n",
        "∣\n",
        "𝜃\n",
        "𝑄\n",
        ")\n",
        "∇\n",
        "𝜃\n",
        "𝜇\n",
        "𝜇\n",
        "(\n",
        "𝑠\n",
        "∣\n",
        "𝜃\n",
        "𝜇\n",
        ")\n",
        "]\n",
        ",\n",
        "∇\n",
        "θ\n",
        "μ\n",
        "​\n",
        "\n",
        "​\n",
        " J≈E[∇\n",
        "a\n",
        "​\n",
        " Q(s,a∣θ\n",
        "Q\n",
        "​\n",
        " )∇\n",
        "θ\n",
        "μ\n",
        "​\n",
        "\n",
        "​\n",
        " μ(s∣θ\n",
        "μ\n",
        "​\n",
        " )],\n",
        "что интуитивно означает улучшение стратегии так, чтобы увеличивать Q-значения выбранных действий."
      ],
      "metadata": {
        "id": "AE9qPiA58wU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Установка библиотек**"
      ],
      "metadata": {
        "id": "gnxvalnS97_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "устанавливка необходимых библиотек: gymnasium с поддержкой box2d для среды CarRacing, а также matplotlib, torch, torchvision и opencv-python для нейронных сетей и обработки изображений."
      ],
      "metadata": {
        "id": "jdSXGgAk-A5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1SCBSNUMJ_m",
        "outputId": "d4446b52-f149-4e52-d4a9-9977b0ed7d4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRjKs0dCCYFX",
        "outputId": "98dbd3e0-33de-4854-d1d4-ce40f2ce9c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install matplotlib torch torchvision opencv-python\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем все необходимые модули.\n",
        "Создаем среду CarRacing-v3 с непрерывным пространством действий.\n",
        "Сбрасываем среду и выводим размеры пространств состояний и действий для проверки.\n",
        "Закрываем среду после проверки."
      ],
      "metadata": {
        "id": "Tw3jNstK-TO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2  # Для предварительной обработки изображений\n",
        "\n",
        "# Проверяем работу среды\n",
        "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=True)\n",
        "obs, info = env.reset()\n",
        "print(\"Observation Space:\", env.observation_space)\n",
        "print(\"Action Space:\", env.action_space)\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_0WuDEJHNUa",
        "outputId": "4ea78ab4-277a-4845-b093-be027b04c791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
            "Action Space: Box([-1.  0.  0.], 1.0, (3,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Повторно создаем среду CarRacing-v3.\n",
        "Определяем форму состояния после преобразования изображения в одноканальное (градации серого) и уменьшения размера до 64x64.\n",
        "Определяем размерность действия (обычно 3: поворот, газ, тормоз) и максимальное действие (1.0).\n",
        "Выводим эти значения для проверки.\n"
      ],
      "metadata": {
        "id": "Ak2F2MvP-dDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Создаем среду\n",
        "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=True)\n",
        "\n",
        "# Определяем форму состояния после предварительной обработки\n",
        "state_shape = (1, 64, 64)  # Одноканальное изображение размера 64x64\n",
        "\n",
        "# Получаем размерность действий и максимальное значение действия из среды\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = env.action_space.high[0]\n",
        "\n",
        "print(f\"State Shape: {state_shape}\")\n",
        "print(f\"Action Dimension: {action_dim}\")\n",
        "print(f\"Max Action: {max_action}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfS0sDiLMvfO",
        "outputId": "492ed59d-52d3-401e-c404-cf2334b01821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Shape: (1, 64, 64)\n",
            "Action Dimension: 3\n",
            "Max Action: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Устанавливаем дополнительные системные пакеты, которые могут потребоваться для сборки некоторых зависимостей. Обычно в Colab или подобных средах это может быть не обязательно, но на локальной машине может пригодиться."
      ],
      "metadata": {
        "id": "Hl5CS3jt-l_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n"
      ],
      "metadata": {
        "id": "VRR7220nX70X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y swig build-essential cmake\n",
        "!pip install swig\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McloVCcSL_42",
        "outputId": "f2a3ca85-f114-4f5d-b848-cdd2815c9274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (754 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.2.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это класс буфера воспроизведения, в котором хранится опыт агента. При обучении мы будем брать из него случайные переходы, чтобы избежать корреляций и улучшить стабильность обучения."
      ],
      "metadata": {
        "id": "K9co7L1n-1pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards)\n",
        "        dones = np.array(dones)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "ZWBVdpkcNAR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция уменьшает размер входного кадра, делает его одноканальным и нормализует. Это уменьшает вычислительную нагрузку и упрощает извлечение признаков из изображения."
      ],
      "metadata": {
        "id": "A-vzorCt-8E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_state(state):\n",
        "    \"\"\"\n",
        "    Преобразует состояние в тензор с уменьшенным размером изображения и в оттенках серого.\n",
        "    \"\"\"\n",
        "    state = cv2.resize(state, (64, 64))  # Изменение размера изображения\n",
        "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)  # Преобразование в оттенки серого\n",
        "    state = state / 255.0  # Нормализация\n",
        "    state = np.expand_dims(state, axis=0)  # Добавляем канал (1, 64, 64)\n",
        "    return torch.tensor(state, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "jm1jJVYDWLoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Акторная сеть обрабатывает изображение, извлекает признаки и предсказывает действие в непрерывном пространстве. Использование свёрток позволяет сети самостоятельно учить важные визуальные признаки."
      ],
      "metadata": {
        "id": "GZPwjZHu-9IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Критик оцениват качество действия в данном состоянии, выдавая Q-значение. Это позволяет актору улучшать свою стратегию."
      ],
      "metadata": {
        "id": "alUFnGxq_PRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_shape, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "\n",
        "        # Вычисляем размер выхода после свёрток\n",
        "        self._init_conv_output(state_shape)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.conv_output_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def _init_conv_output(self, shape):\n",
        "        with torch.no_grad():\n",
        "            input = torch.zeros(1, *shape)\n",
        "            x = F.relu(self.conv1(input))\n",
        "            x = F.relu(self.conv2(x))\n",
        "            self.conv_output_dim = x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.conv1(state))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(-1, self.conv_output_dim)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x)) * self.max_action\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_shape, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "\n",
        "        # Вычисляем размер выхода после свёрток\n",
        "        self._init_conv_output(state_shape)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.conv_output_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "\n",
        "    def _init_conv_output(self, shape):\n",
        "        with torch.no_grad():\n",
        "            input = torch.zeros(1, *shape)\n",
        "            x = F.relu(self.conv1(input))\n",
        "            x = F.relu(self.conv2(x))\n",
        "            self.conv_output_dim = x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = F.relu(self.conv1(state))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(-1, self.conv_output_dim)\n",
        "        x = torch.cat([x, action], dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "RdYWuaqJWRgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Агент реализует DDPG. Он использует буфер, целевые сети и мягкое обновление. Есть методы для сохранения и загрузки моделей."
      ],
      "metadata": {
        "id": "6i3GJMlh_as5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPGAgent:\n",
        "    def __init__(self, state_shape, action_dim, max_action):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.action_dim = action_dim\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.actor = Actor(state_shape, action_dim, max_action).to(self.device)\n",
        "        self.actor_target = Actor(state_shape, action_dim, max_action).to(self.device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "\n",
        "        self.critic = Critic(state_shape, action_dim).to(self.device)\n",
        "        self.critic_target = Critic(state_shape, action_dim).to(self.device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(100000)\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "\n",
        "        self.best_avg_reward = -np.inf\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = preprocess_state(state).unsqueeze(0).to(self.device)  # Добавляем batch размерность\n",
        "        self.actor.eval()  # Переводим модель в режим оценки\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state).cpu().numpy()[0]\n",
        "        self.actor.train()  # Возвращаем модель в режим тренировки\n",
        "        # Добавляем шум для исследования\n",
        "        noise = np.random.normal(0, self.max_action * 0.1, size=self.action_dim)\n",
        "        action = action + noise\n",
        "        action = np.clip(action, -self.max_action, self.max_action)\n",
        "        return action\n",
        "\n",
        "    def train(self, batch_size):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        # Извлекаем батч из реплей буфера\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        # Преобразуем списки тензоров в тензоры батча\n",
        "        states = torch.stack(states).to(self.device)\n",
        "        next_states = torch.stack(next_states).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # Обновление Critic\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.actor_target(next_states)\n",
        "            target_Q = self.critic_target(next_states, next_actions)\n",
        "            target_Q = rewards + (1 - dones) * self.gamma * target_Q\n",
        "\n",
        "        current_Q = self.critic(states, actions)\n",
        "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Обновление Actor\n",
        "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Мягкое обновление целевых сетей\n",
        "        self.soft_update(self.actor_target, self.actor)\n",
        "        self.soft_update(self.critic_target, self.critic)\n",
        "\n",
        "    def soft_update(self, target_net, source_net):\n",
        "        for target_param, param in zip(target_net.parameters(), source_net.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "    def save_models(self, episode):\n",
        "        torch.save(self.actor.state_dict(), os.path.join(model_dir, f'ddpg_actor_episode_{episode}.pth'))\n",
        "        torch.save(self.critic.state_dict(), os.path.join(model_dir, f'ddpg_critic_episode_{episode}.pth'))\n",
        "        print(f\"Models saved at episode {episode}\")\n",
        "\n",
        "    def load_models(self, actor_path, critic_path):\n",
        "        self.actor.load_state_dict(torch.load(os.path.join(model_dir, actor_path)))\n",
        "        self.critic.load_state_dict(torch.load(os.path.join(model_dir, critic_path)))\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        print(\"Models loaded successfully\")\n",
        "\n",
        "    def save_models(self, episode):\n",
        "        torch.save({\n",
        "        'actor_state_dict': self.actor.state_dict(),\n",
        "        'critic_state_dict': self.critic.state_dict(),\n",
        "        'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
        "        'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
        "        }, os.path.join(model_dir, f'ddpg_checkpoint_episode_{episode}.pth'))\n",
        "        print(f\"Models and optimizers saved at episode {episode}\")\n",
        "    def load_models(self, checkpoint_path):\n",
        "        checkpoint = torch.load(os.path.join(model_dir, checkpoint_path))\n",
        "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
        "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
        "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
        "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        print(\"Models and optimizers loaded successfully\")\n",
        "\n"
      ],
      "metadata": {
        "id": "q9EdfgutWUOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/DDPG AGENT'\n",
        "os.makedirs(model_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "V8zeRsnPW3t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем директорию для сохранения моделей. Если вы работаете в Google Colab и подключили Google Drive, это создаст папку в вашем облачном хранилище.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y92VpYqGVK6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pjkKHA1jVKzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполняется основной цикл обучения на 500 эпизодов.\n",
        "Для каждого эпизода агент взаимодействует со средой, собирает награды и сохраняет переходы в буфер.\n",
        "Обновление актёра и критика выполняется после набора достаточного количества переходов.\n",
        "Каждые 5 эпизодов печатается средняя награда, чтобы отслеживать прогресс обучения."
      ],
      "metadata": {
        "id": "BOBs57pXVKvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация агента\n",
        "agent = DDPGAgent(state_shape, action_dim, max_action)\n",
        "episodes = 500  # Можно увеличить количество эпизодов для лучшего обучения\n",
        "batch_size = 16\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "    steps = 0  # Счетчик шагов в эпизоде\n",
        "\n",
        "    while not done:\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Предобрабатываем состояния\n",
        "        state_processed = preprocess_state(state)\n",
        "        next_state_processed = preprocess_state(next_state)\n",
        "\n",
        "        agent.replay_buffer.add(state_processed, action, reward, next_state_processed, done)\n",
        "        agent.train(batch_size)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "        if steps >= 1000:\n",
        "            # Ограничиваем максимальное количество шагов в эпизоде для ускорения обучения\n",
        "            break\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "    if (episode + 1) % 5 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-5:])\n",
        "        print(f\"Episode {episode + 1}, Average Reward: {avg_reward:.2f}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "# После обучения построение графика\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Training Reward per Episode')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Op3ZFBWbzF",
        "outputId": "3a88c352-a1b9-4068-d17e-dd6aa8f2ffb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 5, Average Reward: -81.57\n",
            "Episode 10, Average Reward: -77.91\n",
            "Episode 15, Average Reward: -82.39\n",
            "Episode 20, Average Reward: -81.82\n",
            "Episode 25, Average Reward: -64.67\n",
            "Episode 30, Average Reward: -57.68\n",
            "Episode 35, Average Reward: -61.23\n",
            "Episode 40, Average Reward: -58.13\n",
            "Episode 45, Average Reward: -27.66\n",
            "Episode 50, Average Reward: -36.34\n",
            "Episode 55, Average Reward: -58.34\n",
            "Episode 60, Average Reward: -33.99\n",
            "Episode 65, Average Reward: -44.32\n",
            "Episode 70, Average Reward: -15.60\n",
            "Episode 75, Average Reward: -37.79\n",
            "Episode 80, Average Reward: -8.40\n",
            "Episode 85, Average Reward: -5.61\n",
            "Episode 90, Average Reward: -44.17\n",
            "Episode 95, Average Reward: -27.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Построение графика награды по эпизодам\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(episode_rewards, label='Episode Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Training Reward per Episode')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "O_dLliECL62F",
        "outputId": "f8dbc05e-3955-4e1c-a60c-2d98e0577694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8ea21dbaca9e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Построение графика награды по эпизодам\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Episode Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio_ffmpeg\n",
        "!pip install imageio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_o9-AaAMCb5",
        "outputId": "322b4b13-f508-45fe-9365-fc8dde86fa99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio_ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg) (75.1.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "# Указываем путь для сохранения видео\n",
        "video_folder = 'videos'\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Создаем среду с обёрткой RecordVideo\n",
        "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=True)\n",
        "env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda episode_id: True)\n",
        "\n",
        "state, _ = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "agent.actor.eval()  # Переводим модель в режим оценки\n",
        "\n",
        "while not done:\n",
        "    state_processed = preprocess_state(state).unsqueeze(0).to(agent.device)\n",
        "    with torch.no_grad():\n",
        "        action = agent.actor(state_processed).cpu().numpy()[0]\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    total_reward += reward\n",
        "\n",
        "env.close()\n",
        "print(f\"Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhO1hOtFMF-5",
        "outputId": "b6eb857d-4879-42ef-c114-62e213ea6f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: -85.13011152416297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Функция для отображения видео внутри ноутбука\n",
        "def show_video(video_path):\n",
        "    video = io.open(video_path, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    HTML(data='''\n",
        "        <video width=\"640\" height=\"480\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "    '''.format(encoded.decode('ascii')))\n",
        "\n",
        "# Получаем путь к последнему записанному видео\n",
        "import glob\n",
        "list_of_videos = glob.glob(os.path.join(video_folder, '*.mp4'))\n",
        "latest_video = max(list_of_videos, key=os.path.getctime)\n",
        "\n",
        "# Отображаем видео\n",
        "show_video(latest_video)\n"
      ],
      "metadata": {
        "id": "VIQMP2slMWzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 3  # Количество эпизодов для записи\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    agent.actor.eval()  # Переводим модель в режим оценки\n",
        "\n",
        "    while not done:\n",
        "        state_processed = preprocess_state(state).unsqueeze(0).to(agent.device)\n",
        "        with torch.no_grad():\n",
        "            action = agent.actor(state_processed).cpu().numpy()[0]\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpYJHnYkMfHV",
        "outputId": "33d5f28c-cc5d-4e9d-e5b9-f3aa8d3979f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: -83.27759197324364\n",
            "Episode 2, Total Reward: -83.05084745762663\n",
            "Episode 3, Total Reward: -83.27759197324364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Выводим список всех видео\n",
        "for idx, video_path in enumerate(list_of_videos):\n",
        "    print(f\"{idx + 1}: {video_path}\")\n",
        "\n",
        "# Вводим номер видео для отображения\n",
        "video_number = int(input(\"Enter the number of the video to display: \")) - 1\n",
        "selected_video = list_of_videos[video_number]\n",
        "\n",
        "# Отображаем выбранное видео\n",
        "show_video(selected_video)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZW6mUDKMj_q",
        "outputId": "ab547359-117b-4fdc-9f16-3452a46e27f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: videos/rl-video-episode-0.mp4\n",
            "Enter the number of the video to display: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T54zLzHMMFIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-BcH2U2nMFFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lh2cE5gYMFA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "eLaTyvRY8DUB",
        "outputId": "a50c1533-c8ac-4e42-fb42-d7099f718603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2eaca526b1e6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9v71CWWUWUBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bWV2Y48SWT-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4bvZNHhkWT7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zp7LUqHbWT3f"
      }
    }
  ]
}